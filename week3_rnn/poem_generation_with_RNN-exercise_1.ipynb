{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 诗歌生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_193136\\3276789700.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_functions_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 强制启用 Eager Execution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)  # 强制启用 Eager Execution\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "start_token = 'bos'\n",
    "end_token = 'eos'\n",
    "\n",
    "def process_dataset(fileName):\n",
    "    examples = []\n",
    "    with open(fileName, 'r',encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            outs = line.strip().split(':')\n",
    "            content = ''.join(outs[1:])\n",
    "            ins = [start_token] + list(content) + [end_token] \n",
    "            if len(ins) > 200:\n",
    "                continue\n",
    "            examples.append(ins)\n",
    "            \n",
    "    counter = collections.Counter()\n",
    "    for e in examples:\n",
    "        for w in e:\n",
    "            counter[w]+=1\n",
    "    \n",
    "    sorted_counter = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*sorted_counter)\n",
    "    words = ('PAD', 'UNK') + words[:len(words)]\n",
    "    word2id = dict(zip(words, range(len(words))))\n",
    "    id2word = {word2id[k]:k for k in word2id}\n",
    "    \n",
    "    indexed_examples = [[word2id[w] for w in poem]\n",
    "                        for poem in examples]\n",
    "    seqlen = [len(e) for e in indexed_examples]\n",
    "    \n",
    "    instances = list(zip(indexed_examples, seqlen))\n",
    "    \n",
    "    return instances, word2id, id2word\n",
    "\n",
    "def poem_dataset():\n",
    "    instances, word2id, id2word = process_dataset('poems.txt')\n",
    "    ds = tf.data.Dataset.from_generator(lambda: [ins for ins in instances], \n",
    "                                            (tf.int64, tf.int64), \n",
    "                                            (tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.shuffle(buffer_size=10240)\n",
    "    ds = ds.padded_batch(100, padded_shapes=(tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.map(lambda x, seqlen: (x[:, :-1], x[:, 1:], seqlen-1))\n",
    "    return ds, word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码， 完成建模代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self, w2id):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.v_sz = len(w2id)\n",
    "        self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64, \n",
    "                                                    batch_input_shape=[None, None])\n",
    "        \n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(128)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz)\n",
    "        \n",
    "    #@tf.function\n",
    "    def call(self, inp_ids):\n",
    "        '''\n",
    "        此处完成建模过程，可以参考Learn2Carry\n",
    "        '''\n",
    "        # embedding获取词向量\n",
    "        inp_emb = self.embed_layer(inp_ids)\n",
    "        # rnn处理序列\n",
    "        rnn = self.rnn_layer(inp_emb)\n",
    "        # dense层生成logits\n",
    "        logits = self.dense(rnn)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    #@tf.function\n",
    "    def get_next_token(self, x, state):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        '''\n",
    "    \n",
    "        inp_emb = self.embed_layer(x) #shape(b_sz, emb_sz)\n",
    "        h, state = self.rnncell.call(inp_emb, state) # shape(b_sz, h_sz)\n",
    "        logits = self.dense(h) # shape(b_sz, v_sz)\n",
    "        out = tf.argmax(logits, axis=-1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个计算sequence loss的辅助函数，只需了解用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkMask(input_tensor, maxLen):\n",
    "    shape_of_input = tf.shape(input_tensor)\n",
    "    shape_of_output = tf.concat(axis=0, values=[shape_of_input, [maxLen]])\n",
    "\n",
    "    oneDtensor = tf.reshape(input_tensor, shape=(-1,))\n",
    "    flat_mask = tf.sequence_mask(oneDtensor, maxlen=maxLen)\n",
    "    return tf.reshape(flat_mask, shape_of_output)\n",
    "\n",
    "\n",
    "def reduce_avg(reduce_target, lengths, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reduce_target : shape(d_0, d_1,..,d_dim, .., d_k)\n",
    "        lengths : shape(d0, .., d_(dim-1))\n",
    "        dim : which dimension to average, should be a python number\n",
    "    \"\"\"\n",
    "    shape_of_lengths = lengths.get_shape()\n",
    "    shape_of_target = reduce_target.get_shape()\n",
    "    if len(shape_of_lengths) != dim:\n",
    "        raise ValueError(('Second input tensor should be rank %d, ' +\n",
    "                         'while it got rank %d') % (dim, len(shape_of_lengths)))\n",
    "    if len(shape_of_target) < dim+1 :\n",
    "        raise ValueError(('First input tensor should be at least rank %d, ' +\n",
    "                         'while it got rank %d') % (dim+1, len(shape_of_target)))\n",
    "\n",
    "    rank_diff = len(shape_of_target) - len(shape_of_lengths) - 1\n",
    "    mxlen = tf.shape(reduce_target)[dim]\n",
    "    mask = mkMask(lengths, mxlen)\n",
    "    if rank_diff!=0:\n",
    "        len_shape = tf.concat(axis=0, values=[tf.shape(lengths), [1]*rank_diff])\n",
    "        mask_shape = tf.concat(axis=0, values=[tf.shape(mask), [1]*rank_diff])\n",
    "    else:\n",
    "        len_shape = tf.shape(lengths)\n",
    "        mask_shape = tf.shape(mask)\n",
    "    lengths_reshape = tf.reshape(lengths, shape=len_shape)\n",
    "    mask = tf.reshape(mask, shape=mask_shape)\n",
    "\n",
    "    mask_target = reduce_target * tf.cast(mask, dtype=reduce_target.dtype)\n",
    "\n",
    "    red_sum = tf.reduce_sum(mask_target, axis=[dim], keepdims=False)\n",
    "    red_avg = red_sum / (tf.cast(lengths_reshape, dtype=tf.float32) + 1e-30)\n",
    "    return red_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义loss函数，定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def compute_loss(logits, labels, seqlen):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = reduce_avg(losses, seqlen, dim=1)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "#@tf.function\n",
    "#def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    '''\n",
    "    完成一步优化过程，可以参考之前做过的模型\n",
    "    '''\n",
    "#   with tf.GradientTape() as tape:\n",
    "#       logits = model(x)\n",
    "#       loss = compute_loss(logits,y,seqlen)\n",
    "        \n",
    "#   gradients = tape.gradient(loss,model.trainable_variables)\n",
    "#   optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "#    return loss\n",
    "\n",
    "def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    # 将 NumPy 数组转换为 TensorFlow 张量\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.int32)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "    seqlen = tf.convert_to_tensor(seqlen, dtype=tf.int32)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        logits = model.call(x) \n",
    "        loss = compute_loss(logits, y, seqlen)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        _, loss_val = sess.run([train_op, loss])\n",
    "    return loss_val\n",
    "\n",
    "#def train(epoch, model, optimizer, ds):\n",
    "#    loss = 0.0\n",
    "#    accuracy = 0.0\n",
    "#    for step, (x, y, seqlen) in enumerate(ds):\n",
    "#        loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "\n",
    "#        if step % 500 == 0:\n",
    "#            print('epoch', epoch, ': loss', loss.numpy())\n",
    "\n",
    "#    return loss\n",
    "\n",
    "def train(epoch, model, optimizer, ds):\n",
    "    loss = 0.0\n",
    "    #accuracy = 0\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    next_batch = iterator.get_next()  # 获取下一批数据的操作\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        try:\n",
    "            while True:\n",
    "                x, y, seqlen = sess.run(next_batch)  # 实际获取数据\n",
    "                loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "                if step % 500 == 0:\n",
    "                    print('epoch', epoch, ': loss', loss.numpy())\n",
    "        except tf.errors.OutOfRangeError:  # 数据集遍历完毕\n",
    "            pass\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练优化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable embedding_4/embeddings/Adam from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/embedding_4/embeddings/Adam/class tensorflow::Var does not exist.\n\t [[node Adam_7/update_embedding_4/embeddings/ReadVariableOp_2 (defined at C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py:32) ]]\n\nCaused by op 'Adam_7/update_embedding_4/embeddings/ReadVariableOp_2', defined at:\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1082678462.py\", line 7, in <module>\n    loss = train(epoch, model, optimizer, train_ds)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\", line 57, in train\n    loss = train_one_step(model, optimizer, x, y, seqlen)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\", line 32, in train_one_step\n    train_op = optimizer.minimize(loss)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in minimize\n    name=name)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 612, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 170, in update_op\n    g.values, self._v, g.indices)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 977, in _resource_apply_sparse_duplicate_indices\n    return self._resource_apply_sparse(summed_grad, handle, unique_indices)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 219, in _resource_apply_sparse\n    grad, var, indices, self._resource_scatter_add)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 189, in _apply_sparse_shared\n    m_t = state_ops.assign(m, m * beta1_t,\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 935, in _run_op\n    return tensor_oper(a.value(), *args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 644, in value\n    return self._read_variable_op()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 605, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable embedding_4/embeddings/Adam from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/embedding_4/embeddings/Adam/class tensorflow::Var does not exist.\n\t [[node Adam_7/update_embedding_4/embeddings/ReadVariableOp_2 (defined at C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py:32) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Error while reading resource variable embedding_4/embeddings/Adam from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/embedding_4/embeddings/Adam/class tensorflow::Var does not exist.\n\t [[{{node Adam_7/update_embedding_4/embeddings/ReadVariableOp_2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_193136\\1082678462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, ds)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 实际获取数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\u001b[0m in \u001b[0;36mtrain_one_step\u001b[1;34m(model, optimizer, x, y, seqlen)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Error while reading resource variable embedding_4/embeddings/Adam from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/embedding_4/embeddings/Adam/class tensorflow::Var does not exist.\n\t [[node Adam_7/update_embedding_4/embeddings/ReadVariableOp_2 (defined at C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py:32) ]]\n\nCaused by op 'Adam_7/update_embedding_4/embeddings/ReadVariableOp_2', defined at:\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1082678462.py\", line 7, in <module>\n    loss = train(epoch, model, optimizer, train_ds)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\", line 57, in train\n    loss = train_one_step(model, optimizer, x, y, seqlen)\n  File \"C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py\", line 32, in train_one_step\n    train_op = optimizer.minimize(loss)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in minimize\n    name=name)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 612, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 170, in update_op\n    g.values, self._v, g.indices)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 977, in _resource_apply_sparse_duplicate_indices\n    return self._resource_apply_sparse(summed_grad, handle, unique_indices)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 219, in _resource_apply_sparse\n    grad, var, indices, self._resource_scatter_add)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 189, in _apply_sparse_shared\n    m_t = state_ops.assign(m, m * beta1_t,\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 935, in _run_op\n    return tensor_oper(a.value(), *args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 644, in value\n    return self._read_variable_op()\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 605, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\nndl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable embedding_4/embeddings/Adam from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/embedding_4/embeddings/Adam/class tensorflow::Var does not exist.\n\t [[node Adam_7/update_embedding_4/embeddings/ReadVariableOp_2 (defined at C:\\Users\\xinling\\AppData\\Local\\Temp\\ipykernel_193136\\1562360009.py:32) ]]\n"
     ]
    }
   ],
   "source": [
    "#optimizer = optimizers.Adam(0.0005)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "train_ds, word2id, id2word = poem_dataset()\n",
    "model = myRNNModel(word2id)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(epoch, model, optimizer, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一年一里，花落花声。eosPAD山风雨，风吹落月。eosPAD山风雨，风吹落月。eosPAD山风雨，风吹落月。eosPAD山风雨，风\n"
     ]
    }
   ],
   "source": [
    "def gen_sentence():\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id['bos']], dtype=tf.int32)\n",
    "    collect = []\n",
    "    for _ in range(50):\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        collect.append(cur_token.numpy()[0])\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
